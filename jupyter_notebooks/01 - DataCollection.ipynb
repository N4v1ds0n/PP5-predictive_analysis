{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fc9ffad",
   "metadata": {},
   "source": [
    "# **Data collection Notebook**\n",
    "\n",
    "## Contents and purpose\n",
    "\n",
    "- Import packages\n",
    "- set up directory and path structure\n",
    "- Load raw data from Kaggle and save it to repo\n",
    "- sift through the data and process/ save it respectively\n",
    "- clean data\n",
    "- create train, test and validation sets\n",
    "\n",
    "## Important files\n",
    "\n",
    "- kaggle JSON file is a personal authentication token, if this repo is forked and reproduced, it needs to be replaced by an individual file.\n",
    "\n",
    "## Expected Results\n",
    "\n",
    "- we will receive the necessary data for the subsequent notebooks\n",
    "    - a train set to train our models\n",
    "    - a test set\n",
    "    - a validation set\n",
    "- each set will have healthy and afflicted sample images\n",
    "\n",
    "## Why are we doing this\n",
    "\n",
    "These steps are common practice for the necessary preparation of data sets for machine learning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d87c140",
   "metadata": {},
   "source": [
    "# Install/ Import packages necessary for this notebook\n",
    "\n",
    "- if you have created your working environment based on the requirements.txt file, you can skip the next step, as the requirements will already be satisfied. If not, you cann install the necessary packages now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3255f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f1c96f",
   "metadata": {},
   "source": [
    "Now you can import the packages that will be needed in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "effd5a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65bebd6",
   "metadata": {},
   "source": [
    "## Set working directory and file path architecture for notebook\n",
    "As the notebooks are set in a subfolder of this repo we need to adjust the working directory so files can be accessed properly. \n",
    "\n",
    "First we check our current working directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "599ab9fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'e:\\\\Projects\\\\Code-I\\\\vscode-projects\\\\PP5-predictive_analysis\\\\jupyter_notebooks'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_dir = os.getcwd()\n",
    "current_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb3a592",
   "metadata": {},
   "source": [
    "Now we can change the directory to the parent folder that contains the complete repo. We will also print our new working directory so we can check everything worked out as planned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef4bedca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You set a new current directory:\n",
      "e:\\Projects\\Code-I\\vscode-projects\\PP5-predictive_analysis\n"
     ]
    }
   ],
   "source": [
    "os.chdir('../')\n",
    "print(\"You set a new current directory:\")\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d853810",
   "metadata": {},
   "source": [
    "# Kaggle as a data source\n",
    "\n",
    "Kaggle is a data science platform that offers a vast repository of publicly shared datasets across diverse domains such as healthcare, finance, sports, and more. These datasets are freely available for analysis, modeling, and learning, making Kaggle a popular resource for data scientists and machine learning practitioners.\n",
    "\n",
    "In this repo we will use data from Kaggle and thus it is already part of the requirements file. If you want to install it separately you can do so via pip:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6bb06e61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kaggle in e:\\projects\\code-i\\vscode-projects\\pp5-predictive_analysis\\.venv\\lib\\site-packages (1.5.12)\n",
      "Requirement already satisfied: six>=1.10 in e:\\projects\\code-i\\vscode-projects\\pp5-predictive_analysis\\.venv\\lib\\site-packages (from kaggle) (1.17.0)\n",
      "Requirement already satisfied: certifi in e:\\projects\\code-i\\vscode-projects\\pp5-predictive_analysis\\.venv\\lib\\site-packages (from kaggle) (2025.6.15)\n",
      "Requirement already satisfied: python-dateutil in e:\\projects\\code-i\\vscode-projects\\pp5-predictive_analysis\\.venv\\lib\\site-packages (from kaggle) (2.9.0.post0)\n",
      "Requirement already satisfied: requests in e:\\projects\\code-i\\vscode-projects\\pp5-predictive_analysis\\.venv\\lib\\site-packages (from kaggle) (2.32.4)\n",
      "Requirement already satisfied: tqdm in e:\\projects\\code-i\\vscode-projects\\pp5-predictive_analysis\\.venv\\lib\\site-packages (from kaggle) (4.67.1)\n",
      "Requirement already satisfied: python-slugify in e:\\projects\\code-i\\vscode-projects\\pp5-predictive_analysis\\.venv\\lib\\site-packages (from kaggle) (8.0.4)\n",
      "Requirement already satisfied: urllib3 in e:\\projects\\code-i\\vscode-projects\\pp5-predictive_analysis\\.venv\\lib\\site-packages (from kaggle) (2.5.0)\n",
      "Requirement already satisfied: text-unidecode>=1.3 in e:\\projects\\code-i\\vscode-projects\\pp5-predictive_analysis\\.venv\\lib\\site-packages (from python-slugify->kaggle) (1.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in e:\\projects\\code-i\\vscode-projects\\pp5-predictive_analysis\\.venv\\lib\\site-packages (from requests->kaggle) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in e:\\projects\\code-i\\vscode-projects\\pp5-predictive_analysis\\.venv\\lib\\site-packages (from requests->kaggle) (3.10)\n",
      "Requirement already satisfied: colorama in e:\\projects\\code-i\\vscode-projects\\pp5-predictive_analysis\\.venv\\lib\\site-packages (from tqdm->kaggle) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c497bf5",
   "metadata": {},
   "source": [
    "Once we have installed Kaggle we need to change the Kaggle config directory to our current working directory. We also need to need to authenticate using our kaggle.json file. (Can be obtained from the user settings in your kaggle account)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8991c299",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Der Befehl \"chmod\" ist entweder falsch geschrieben oder\n",
      "konnte nicht gefunden werden.\n"
     ]
    }
   ],
   "source": [
    "# change Kaggle config directory\n",
    "os.environ['KAGGLE_CONFIG_DIR'] = os.getcwd()\n",
    "# Set permissions for kaggle using our json file\n",
    "! chmod 600 kaggle.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e83e7ed2",
   "metadata": {},
   "source": [
    "Now we can obtain our dataset for this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2360cf1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading cherry-leaves.zip to inputs/datasets/raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0.00/55.0M [00:00<?, ?B/s]\n",
      "  2%|▏         | 1.00M/55.0M [00:00<00:33, 1.70MB/s]\n",
      "  5%|▌         | 3.00M/55.0M [00:00<00:11, 4.66MB/s]\n",
      "  9%|▉         | 5.00M/55.0M [00:00<00:07, 6.98MB/s]\n",
      " 13%|█▎        | 7.00M/55.0M [00:01<00:05, 8.71MB/s]\n",
      " 16%|█▋        | 9.00M/55.0M [00:01<00:04, 9.97MB/s]\n",
      " 20%|█▉        | 11.0M/55.0M [00:01<00:04, 10.9MB/s]\n",
      " 24%|██▎       | 13.0M/55.0M [00:01<00:03, 11.6MB/s]\n",
      " 27%|██▋       | 15.0M/55.0M [00:01<00:03, 12.0MB/s]\n",
      " 31%|███       | 17.0M/55.0M [00:01<00:03, 12.4MB/s]\n",
      " 35%|███▍      | 19.0M/55.0M [00:02<00:02, 12.6MB/s]\n",
      " 38%|███▊      | 21.0M/55.0M [00:02<00:02, 12.8MB/s]\n",
      " 42%|████▏     | 23.0M/55.0M [00:02<00:02, 12.9MB/s]\n",
      " 45%|████▌     | 25.0M/55.0M [00:02<00:02, 13.0MB/s]\n",
      " 49%|████▉     | 27.0M/55.0M [00:02<00:02, 13.0MB/s]\n",
      " 53%|█████▎    | 29.0M/55.0M [00:02<00:02, 13.1MB/s]\n",
      " 56%|█████▋    | 31.0M/55.0M [00:03<00:01, 13.1MB/s]\n",
      " 60%|█████▉    | 33.0M/55.0M [00:03<00:01, 13.1MB/s]\n",
      " 64%|██████▎   | 35.0M/55.0M [00:03<00:01, 13.2MB/s]\n",
      " 67%|██████▋   | 37.0M/55.0M [00:03<00:01, 13.2MB/s]\n",
      " 71%|███████   | 39.0M/55.0M [00:03<00:01, 13.2MB/s]\n",
      " 75%|███████▍  | 41.0M/55.0M [00:03<00:01, 13.2MB/s]\n",
      " 78%|███████▊  | 43.0M/55.0M [00:03<00:00, 13.2MB/s]\n",
      " 82%|████████▏ | 45.0M/55.0M [00:04<00:00, 13.2MB/s]\n",
      " 85%|████████▌ | 47.0M/55.0M [00:04<00:00, 13.2MB/s]\n",
      " 89%|████████▉ | 49.0M/55.0M [00:04<00:00, 13.2MB/s]\n",
      " 93%|█████████▎| 51.0M/55.0M [00:04<00:00, 13.2MB/s]\n",
      " 96%|█████████▋| 53.0M/55.0M [00:04<00:00, 13.2MB/s]\n",
      "100%|█████████▉| 55.0M/55.0M [00:04<00:00, 13.2MB/s]\n",
      "100%|██████████| 55.0M/55.0M [00:04<00:00, 11.7MB/s]\n"
     ]
    }
   ],
   "source": [
    "# Set variables to define source and destination of our kaggle dataset\n",
    "data_path = \"codeinstitute/cherry-leaves\"\n",
    "data_folder = \"inputs/datasets/raw\"\n",
    "# If our inputs folder does not exist yet, we are creating it in the next step\n",
    "os.makedirs(data_folder, exist_ok=True)   \n",
    "# Finally we download and save the dataset\n",
    "! kaggle datasets download -d {data_path} -p {data_folder}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e72a7c",
   "metadata": {},
   "source": [
    "Now that we have our raw data, we will unzip it and remove the zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cad8af03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "with zipfile.ZipFile(data_folder + '/cherry-leaves.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall(data_folder)\n",
    "\n",
    "os.remove(data_folder + '/cherry-leaves.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d360e769",
   "metadata": {},
   "source": [
    "# Data processing\n",
    "\n",
    "---\n",
    "\n",
    "## Data cleaning\n",
    "\n",
    "Check for unnecessary files and remove all excess files. A function to remove access files can be found in PP5-predictive_analysis\\src\\data_processing.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d4704be7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder 'healthy': Image files = 2104, Non-image files removed = 0\n",
      "Folder 'powdery_mildew': Image files = 2104, Non-image files removed = 0\n",
      "Folder 'test': Image files = 0, Non-image files removed = 0\n",
      "Folder 'train': Image files = 0, Non-image files removed = 0\n",
      "Folder 'validation': Image files = 0, Non-image files removed = 0\n"
     ]
    }
   ],
   "source": [
    "# First we will add the ressource file to our path to be able to load relevant functions\n",
    "sys.path.append('./src')\n",
    "# Then we load our function from the ressource file\n",
    "from data_processing import remove_non_image_files\n",
    "\n",
    "remove_non_image_files(data_dir='inputs/datasets/raw/cherry-leaves')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82df5648",
   "metadata": {},
   "source": [
    "Now that only image files remain, we should check if all images are in working order or if we have some corrupted images in our data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fed94af9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Total corrupt images removed: 0\n"
     ]
    }
   ],
   "source": [
    "from data_processing import remove_corrupt_images\n",
    "\n",
    "corrupt_images = remove_corrupt_images(\"inputs/datasets/raw/cherry-leaves\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c19b8e",
   "metadata": {},
   "source": [
    "# Split data into train-, test-, and validation set\n",
    "\n",
    "For the upcoming model training, we need a train test to train our model, a validation set to adjust our model training process and a test set to test our models performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "667930ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_processing import split_dataset, clear_splits\n",
    "\n",
    "# First we clear the old splits if they already exist (e.g. if we run this script again to change the ratios)\n",
    "# Note, that you need to reload the original dataset to be able to run this script again\n",
    "clear_splits(data_dir='inputs/datasets/raw/cherry-leaves')\n",
    "\n",
    "# Then we split the dataset into train, validation, and test sets\n",
    "split_dataset(data_dir=f\"inputs/datasets/raw/cherry-leaves\",\n",
    "                                   train_ratio=0.7,\n",
    "                                   validation_ratio=0.15,\n",
    "                                   test_ratio=0.15\n",
    "                                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e8cef2",
   "metadata": {},
   "source": [
    "To get an overview of the size of the sets and to check if the sets are ready for the next steps we will count the data entries of the sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7ba87833",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1472 images in train/healthy\n",
      "Warning: Directory 'inputs/datasets/raw/cherry-leaves\\train\\infected' not found.\n",
      "There are 315 images in validation/healthy\n",
      "Warning: Directory 'inputs/datasets/raw/cherry-leaves\\validation\\infected' not found.\n",
      "There are 317 images in test/healthy\n",
      "Warning: Directory 'inputs/datasets/raw/cherry-leaves\\test\\infected' not found.\n",
      "\n",
      "Total number of images: 2104\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2104"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from data_processing import count_dataset_images\n",
    "\n",
    "\n",
    "sets = ['train', 'validation', 'test']\n",
    "labels = ['healthy', 'infected']\n",
    "base_path = 'inputs/datasets/raw/cherry-leaves'\n",
    "\n",
    "count_dataset_images(base_path, sets, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3117103b",
   "metadata": {},
   "source": [
    "## Summary and Next Steps\n",
    "\n",
    "In this notebook, we performed the essential preprocessing steps to prepare our cherry leaf dataset for modeling:\n",
    "\n",
    "- Removed non-image and corrupt files to ensure data integrity.\n",
    "- Verified and cleaned the directory structure.\n",
    "- Split the dataset into **training**, **validation**, and **test** sets with user-defined ratios. Our       default will be (70%, 15%, 15%)\n",
    "\n",
    "These steps ensure that our dataset is clean, balanced, and ready for model training and evaluation.\n",
    "\n",
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "Now that the dataset has been cleaned and split, the next steps are focused on understanding the data and preparing it for model training so we will explore the data (EDA) and visualize the results:\n",
    "\n",
    "- **Analyze class distribution** to check for potential imbalance between categories.\n",
    "- **Visualize image samples** to assess data quality and variation within classes.\n",
    "- **Inspect image dimensions and aspect ratios** to inform resizing or preprocessing decisions.\n",
    "\n",
    "These steps will help guide decisions around model architecture, data augmentation, and normalization techniques."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
